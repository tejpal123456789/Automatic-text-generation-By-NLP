{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Automatic text generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNtF+zwz3IXoUyvyZP9p4HN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejpal123456789/Automatic-text-generation-By-NLP/blob/main/Automatic_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewcn0LnpPCyB",
        "outputId": "7ff24794-25d4-48dc-bfb4-1fe3ede3b4a8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wawv6QCCPZ7w"
      },
      "source": [
        "response = requests.get('https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "ityIwm7LPiFB",
        "outputId": "df8e5696-f7a1-4f6a-cb67-0f9282ade143"
      },
      "source": [
        "response.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2Q_-vazzPyMt",
        "outputId": "db346403-ca50-48f7-f0c8-8cf99ba18f38"
      },
      "source": [
        "data=response.text.split('\\n')\n",
        "data[253]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  From fairest creatures we desire increase,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZRSvOdZQEX6",
        "outputId": "22ef5a53-ea3e-4225-c958-93986a265efd"
      },
      "source": [
        "data1=data[253:]\n",
        "data1[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  From fairest creatures we desire increase,',\n",
              " \"  That thereby beauty's rose might never die,\",\n",
              " '  But as the riper should by time decease,',\n",
              " '  His tender heir might bear his memory:',\n",
              " '  But thou contracted to thine own bright eyes,',\n",
              " \"  Feed'st thy light's flame with self-substantial fuel,\",\n",
              " '  Making a famine where abundance lies,',\n",
              " '  Thy self thy foe, to thy sweet self too cruel:',\n",
              " \"  Thou that art now the world's fresh ornament,\",\n",
              " '  And only herald to the gaudy spring,']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEtAFCzWR4MW"
      },
      "source": [
        "Tokenizations and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "xVlQ7FlATFXl",
        "outputId": "b7a2bcb3-f9b7-48d3-e3a3-ceba8ef5f800"
      },
      "source": [
        "data2=\" \".join(data1)\n",
        "data2[:1000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"  From fairest creatures we desire increase,   That thereby beauty's rose might never die,   But as the riper should by time decease,   His tender heir might bear his memory:   But thou contracted to thine own bright eyes,   Feed'st thy light's flame with self-substantial fuel,   Making a famine where abundance lies,   Thy self thy foe, to thy sweet self too cruel:   Thou that art now the world's fresh ornament,   And only herald to the gaudy spring,   Within thine own bud buriest thy content,   And tender churl mak'st waste in niggarding:     Pity the world, or else this glutton be,     To eat the world's due, by the grave and thee.                        2   When forty winters shall besiege thy brow,   And dig deep trenches in thy beauty's field,   Thy youth's proud livery so gazed on now,   Will be a tattered weed of small worth held:     Then being asked, where all thy beauty lies,   Where all the treasure of thy lusty days;   To say within thine own deep sunken eyes,   Were an all\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYMGfiJBQTmI",
        "outputId": "2a93d28c-9f54-48ad-f590-e2e5b484f3a5"
      },
      "source": [
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl=WordNetLemmatizer()\n",
        "import re\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZbrcuj4SXS7"
      },
      "source": [
        "\n",
        "def clean_text(data):\n",
        "    corpus=[]\n",
        "    review=re.sub('[^a-zA-z]',' ',data)\n",
        "    \n",
        "    \n",
        "    review=review.lower()\n",
        "    review=review.split()\n",
        "    \n",
        "    #review=[wnl.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    \n",
        "    corpus.append(review)\n",
        "#print(corpus)    \n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSHV3c-vSoAf"
      },
      "source": [
        "cleaned_data=clean_text(data2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8v6-EKEXQ8M",
        "outputId": "04ec63b7-127b-43c9-9965-42cf8df3c09b"
      },
      "source": [
        "print(cleaned_data[:1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1zW94LIZYja",
        "outputId": "70729fb3-68a5-48ff-9969-22eb29750c1e"
      },
      "source": [
        "import string\n",
        "def clean_text(doc):\n",
        "  tokens = doc.split()\n",
        "  table = str.maketrans('', '', string.punctuation)  \n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens\n",
        "\n",
        "tokens = clean_text(data2)\n",
        "print(tokens[:50])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['from', 'fairest', 'creatures', 'we', 'desire', 'increase', 'that', 'thereby', 'beautys', 'rose', 'might', 'never', 'die', 'but', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', 'his', 'tender', 'heir', 'might', 'bear', 'his', 'memory', 'but', 'thou', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes', 'feedst', 'thy', 'lights', 'flame', 'with', 'selfsubstantial', 'fuel', 'making', 'a', 'famine', 'where', 'abundance', 'lies', 'thy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBFgalOPZsvu",
        "outputId": "8b9f48bf-eaf9-4b93-9e24-b2ca3568251b"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "898199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IueT1-k8ZzW3",
        "outputId": "e1ffaacb-f5d5-4098-8c95-3ee2647fe31e"
      },
      "source": [
        "len(set(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27956"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEzg_PR4_6S9"
      },
      "source": [
        "Now we need to make dataset for training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypUH2bMdNBpQ"
      },
      "source": [
        "Now we will define the length of our input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlNQFBhwNAIy"
      },
      "source": [
        "length=50+1\n",
        "lines=[]\n",
        "\n",
        "for i in range(length,len(tokens)):\n",
        "    input_data=tokens[i-length:i]\n",
        "    input=' '.join(input_data)\n",
        "    lines.append(input)\n",
        "    if i>10000:\n",
        "        break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "CgbIEK-G_5d2",
        "outputId": "efe13d9a-c20e-43e4-df46-79244b94b00d"
      },
      "source": [
        "lines[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from fairest creatures we desire increase that thereby beautys rose might never die but as the riper should by time decease his tender heir might bear his memory but thou contracted to thine own bright eyes feedst thy lights flame with selfsubstantial fuel making a famine where abundance lies thy self'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZFFYLtqZ154",
        "outputId": "e945b1eb-8ed1-4897-aede-b69f0bf7e49a"
      },
      "source": [
        "len(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9951"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G13l2VOhOzHB"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaFh13yrO15l"
      },
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences=tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oORDhf2bQ8vL"
      },
      "source": [
        "sequences=np.array(sequences\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3evuPxzQrmg",
        "outputId": "319c7e16-a1b6-4c43-d12b-7f4e0bccf28f"
      },
      "source": [
        "X, y = sequences[:, :-1], sequences[:,-1]\n",
        "X[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  34, 2238, 2237,  262,  218,  541,    8,  871,  128,  540,  148,\n",
              "        392,  164,   14,   27,    2, 2232,   54,   30,   46,  870,   25,\n",
              "        305,  869,  148,  163,   25,  217,   14,   10,  868,    4,   56,\n",
              "         77,  149,   52, 2229,    9, 2228, 2227,   11, 2226, 2225,  216,\n",
              "         15, 2223,   53,  538,  215,    9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw5r8IL6REOY",
        "outputId": "9592ce81-cc0f-4d0a-9b1a-97895854a4b9"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2239"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G6LME_yRLvP",
        "outputId": "d28577be-4581-4372-d1c0-250c330ae22e"
      },
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "print(X.shape[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhL4D88wRnhK"
      },
      "source": [
        "seq_length=X.shape[1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IxthhiiSgtd"
      },
      "source": [
        "LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b0QEQoUSiwK",
        "outputId": "1b10b754-32b8-4c56-c707-675d8d0343f5"
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size,50,input_length=seq_length))\n",
        "model.add(LSTM(256,return_sequences=True))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(vocab_size,activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 50, 50)            111950    \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 50, 256)           314368    \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2239)              575423    \n",
            "=================================================================\n",
            "Total params: 1,527,053\n",
            "Trainable params: 1,527,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6assh7p4U7WH"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoCJSuq6T_aZ",
        "outputId": "a1e4303e-ed0b-4479-d0c8-bd9a52c59295"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.fit(X,y,batch_size=50,validation_split=0.2,epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "160/160 [==============================] - 6s 21ms/step - loss: 5.5895 - accuracy: 0.0534 - val_loss: 7.1723 - val_accuracy: 0.0301\n",
            "Epoch 2/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 5.4393 - accuracy: 0.0550 - val_loss: 7.4254 - val_accuracy: 0.0331\n",
            "Epoch 3/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 5.4211 - accuracy: 0.0522 - val_loss: 7.5946 - val_accuracy: 0.0306\n",
            "Epoch 4/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 5.1597 - accuracy: 0.0627 - val_loss: 7.6864 - val_accuracy: 0.0291\n",
            "Epoch 5/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 5.0854 - accuracy: 0.0696 - val_loss: 7.8298 - val_accuracy: 0.0246\n",
            "Epoch 6/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 4.9888 - accuracy: 0.0605 - val_loss: 8.0452 - val_accuracy: 0.0291\n",
            "Epoch 7/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 5.1905 - accuracy: 0.0610 - val_loss: 7.7065 - val_accuracy: 0.0246\n",
            "Epoch 8/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 4.9518 - accuracy: 0.0628 - val_loss: 7.8845 - val_accuracy: 0.0276\n",
            "Epoch 9/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 4.7163 - accuracy: 0.0751 - val_loss: 8.1343 - val_accuracy: 0.0196\n",
            "Epoch 10/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 4.5191 - accuracy: 0.0868 - val_loss: 8.3381 - val_accuracy: 0.0256\n",
            "Epoch 11/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 4.3574 - accuracy: 0.0992 - val_loss: 8.5250 - val_accuracy: 0.0251\n",
            "Epoch 12/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 4.1746 - accuracy: 0.1232 - val_loss: 8.7094 - val_accuracy: 0.0236\n",
            "Epoch 13/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 4.0126 - accuracy: 0.1525 - val_loss: 8.8392 - val_accuracy: 0.0246\n",
            "Epoch 14/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 3.8071 - accuracy: 0.1864 - val_loss: 8.9835 - val_accuracy: 0.0206\n",
            "Epoch 15/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 3.6150 - accuracy: 0.2293 - val_loss: 9.0972 - val_accuracy: 0.0231\n",
            "Epoch 16/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 3.4279 - accuracy: 0.2728 - val_loss: 9.2105 - val_accuracy: 0.0206\n",
            "Epoch 17/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 3.1986 - accuracy: 0.3204 - val_loss: 9.3431 - val_accuracy: 0.0191\n",
            "Epoch 18/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 3.1054 - accuracy: 0.3522 - val_loss: 9.3765 - val_accuracy: 0.0176\n",
            "Epoch 19/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 2.8796 - accuracy: 0.3802 - val_loss: 9.4823 - val_accuracy: 0.0201\n",
            "Epoch 20/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 2.6760 - accuracy: 0.4317 - val_loss: 9.5945 - val_accuracy: 0.0246\n",
            "Epoch 21/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 2.5447 - accuracy: 0.4522 - val_loss: 9.6771 - val_accuracy: 0.0141\n",
            "Epoch 22/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 2.3230 - accuracy: 0.5051 - val_loss: 9.7921 - val_accuracy: 0.0186\n",
            "Epoch 23/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 2.1913 - accuracy: 0.5382 - val_loss: 9.8670 - val_accuracy: 0.0156\n",
            "Epoch 24/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 2.0049 - accuracy: 0.5859 - val_loss: 9.9578 - val_accuracy: 0.0176\n",
            "Epoch 25/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 1.8886 - accuracy: 0.6094 - val_loss: 10.0563 - val_accuracy: 0.0156\n",
            "Epoch 26/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 1.7622 - accuracy: 0.6401 - val_loss: 10.1446 - val_accuracy: 0.0211\n",
            "Epoch 27/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 1.6450 - accuracy: 0.6703 - val_loss: 10.2136 - val_accuracy: 0.0176\n",
            "Epoch 28/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 1.4935 - accuracy: 0.7109 - val_loss: 10.3094 - val_accuracy: 0.0176\n",
            "Epoch 29/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 1.3629 - accuracy: 0.7395 - val_loss: 10.3864 - val_accuracy: 0.0201\n",
            "Epoch 30/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 1.2784 - accuracy: 0.7693 - val_loss: 10.4810 - val_accuracy: 0.0211\n",
            "Epoch 31/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 1.1789 - accuracy: 0.7930 - val_loss: 10.5346 - val_accuracy: 0.0171\n",
            "Epoch 32/50\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 1.0661 - accuracy: 0.8238 - val_loss: 10.6151 - val_accuracy: 0.0191\n",
            "Epoch 33/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.9785 - accuracy: 0.8445 - val_loss: 10.7419 - val_accuracy: 0.0176\n",
            "Epoch 34/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.8792 - accuracy: 0.8731 - val_loss: 10.8208 - val_accuracy: 0.0186\n",
            "Epoch 35/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.7940 - accuracy: 0.8930 - val_loss: 10.8777 - val_accuracy: 0.0176\n",
            "Epoch 36/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.7183 - accuracy: 0.9064 - val_loss: 10.9770 - val_accuracy: 0.0201\n",
            "Epoch 37/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.6297 - accuracy: 0.9321 - val_loss: 11.0383 - val_accuracy: 0.0136\n",
            "Epoch 38/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5665 - accuracy: 0.9412 - val_loss: 11.1300 - val_accuracy: 0.0171\n",
            "Epoch 39/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5114 - accuracy: 0.9574 - val_loss: 11.2351 - val_accuracy: 0.0191\n",
            "Epoch 40/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.4533 - accuracy: 0.9643 - val_loss: 11.2583 - val_accuracy: 0.0196\n",
            "Epoch 41/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.3926 - accuracy: 0.9745 - val_loss: 11.3664 - val_accuracy: 0.0186\n",
            "Epoch 42/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.3484 - accuracy: 0.9832 - val_loss: 11.4606 - val_accuracy: 0.0191\n",
            "Epoch 43/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.2979 - accuracy: 0.9902 - val_loss: 11.5198 - val_accuracy: 0.0161\n",
            "Epoch 44/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.2593 - accuracy: 0.9930 - val_loss: 11.5871 - val_accuracy: 0.0181\n",
            "Epoch 45/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.2237 - accuracy: 0.9957 - val_loss: 11.6673 - val_accuracy: 0.0186\n",
            "Epoch 46/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.1955 - accuracy: 0.9970 - val_loss: 11.7407 - val_accuracy: 0.0171\n",
            "Epoch 47/50\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.1775 - accuracy: 0.9966 - val_loss: 11.8176 - val_accuracy: 0.0201\n",
            "Epoch 48/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.1449 - accuracy: 0.9986 - val_loss: 11.8968 - val_accuracy: 0.0176\n",
            "Epoch 49/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.1293 - accuracy: 1.0000 - val_loss: 11.9515 - val_accuracy: 0.0186\n",
            "Epoch 50/50\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.1093 - accuracy: 0.9998 - val_loss: 12.0090 - val_accuracy: 0.0156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f464e9c4f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EVMR6fwAbKE"
      },
      "source": [
        "Now we will generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPD3hjgoWLTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948cd314-6972-455f-933c-bd46e215478f"
      },
      "source": [
        "seed_text=lines[1000]\n",
        "seed_text\n",
        "encoded=tokenizer.texts_to_sequences([seed_text])[0]\n",
        "print(encoded)\n",
        "print(len(encoded))\n",
        "encoded = pad_sequences([encoded], maxlen = 50)\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[48, 10, 276, 10, 41, 598, 3, 154, 14, 8, 10, 200, 582, 16, 119, 1019, 12, 10, 41, 19, 599, 11, 595, 430, 8, 278, 9, 32, 10, 1020, 24, 4, 1021, 1022, 8, 166, 1023, 4, 1024, 29, 4, 400, 54, 21, 9, 600, 218, 59, 237, 9, 100]\n",
            "51\n",
            "[[  10  276   10   41  598    3  154   14    8   10  200  582   16  119\n",
            "  1019   12   10   41   19  599   11  595  430    8  278    9   32   10\n",
            "  1020   24    4 1021 1022    8  166 1023    4 1024   29    4  400   54\n",
            "    21    9  600  218   59  237    9  100]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Muja6E2wAsnQ"
      },
      "source": [
        "def generate_text(seed_text,tokenizer,n_words,text_seq_length):    \n",
        "    text=[]\n",
        "    for _ in range(n_words):\n",
        "         encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "         encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n",
        "\n",
        "         predicted_index=model.predict_classes(encoded)\n",
        "         predicted_word=''\n",
        "\n",
        "         for word,index in tokenizer.word_index.items():\n",
        "             if predicted_index==index:\n",
        "                 predicted_word=word\n",
        "                 break\n",
        "         seed_text=seed_text+' '+predicted_word\n",
        "         text.append(predicted_word)\n",
        "    return text "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMT8_gj-F4A1",
        "outputId": "7fccdff9-cefd-46c4-ac19-6c602fb0ff6c"
      },
      "source": [
        "generate_text(seed_text, tokenizer,40\n",
        "              ,50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['that',\n",
              " 'i',\n",
              " 'may',\n",
              " 'change',\n",
              " 'my',\n",
              " 'mind',\n",
              " 'shall',\n",
              " 'hate',\n",
              " 'be',\n",
              " 'fairer',\n",
              " 'lodged',\n",
              " 'than',\n",
              " 'gentle',\n",
              " 'love',\n",
              " 'be',\n",
              " 'as',\n",
              " 'thy',\n",
              " 'presence',\n",
              " 'is',\n",
              " 'gracious',\n",
              " 'and',\n",
              " 'kind',\n",
              " 'or',\n",
              " 'to',\n",
              " 'thy',\n",
              " 'self',\n",
              " 'at',\n",
              " 'least',\n",
              " 'kindhearted',\n",
              " 'prove',\n",
              " 'make',\n",
              " 'thee',\n",
              " 'another',\n",
              " 'self',\n",
              " 'for',\n",
              " 'love',\n",
              " 'of',\n",
              " 'me',\n",
              " 'that',\n",
              " 'beauty']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW-mT-3UGPs1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}